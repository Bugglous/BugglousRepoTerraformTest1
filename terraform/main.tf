terraform {
  backend "s3" {
    bucket = "bugglous-state"
    key    = "terraform.tfstate"
    region = "us-east-1"
  }
}
provider "aws" {
  region = "us-east-1"
}

# terraform {
  
# }

# provider "kubernetes" {
#   host                   = data.aws_eks_cluster.cluster.endpoint
#   cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
#   token                  = data.aws_eks_cluster_auth.cluster.token
#   load_config_file       = false
#   version                = "~> 1.13.4"
# }

# locals {
#   cluster_name = "my-cluster-bugglous"
# }

# data "aws_availability_zones" "available" {
# }

# # data "aws_eks_cluster" "cluster" {
# #  name = module.eks.cluster_id
# #}

# #data "aws_eks_cluster_auth" "cluster" {
# #  name = module.eks.cluster_id
# #}


# module "vpc" {
#   source  = "terraform-aws-modules/vpc/aws"
#   version = "3.13.0"

#   name                 = "k8s-vpc"
#   cidr                 = "172.16.0.0/16"
#   azs                  = data.aws_availability_zones.available.names
#   private_subnets      = ["172.16.1.0/24", "172.16.2.0/24", "172.16.3.0/24"]
#   public_subnets       = ["172.16.4.0/24", "172.16.5.0/24", "172.16.6.0/24"]
#   enable_nat_gateway   = true
#   single_nat_gateway   = true
#   enable_dns_hostnames = true

#   public_subnet_tags = {
#     "kubernetes.io/cluster/${local.cluster_name}" = "shared"
#     "kubernetes.io/role/elb"                      = "1"
#   }

#   private_subnet_tags = {
#     "kubernetes.io/cluster/${local.cluster_name}" = "shared"
#     "kubernetes.io/role/internal-elb"             = "1"
#   }
# }

# # module "eks" {
# #   source  = "terraform-aws-modules/eks/aws"
# #   version = "18.11.0"

# #   cluster_name    = "${local.cluster_name}"
# #   cluster_version = "1.17"
# #   subnet_ids         = [module.vpc.private_subnets]

# #   vpc_id = module.vpc.vpc_id
# # }
# output "cluster_id" {
#   description = "EKS cluster ID."
#   value       = module.eks.cluster_id
# }

# output "cluster_endpoint" {
#   description = "Endpoint for EKS control plane."
#   value       = module.eks.cluster_endpoint
# }

# output "cluster_security_group_id" {
#   description = "Security group ids attached to the cluster control plane."
#   value       = module.eks.cluster_security_group_id
# }

# output "kubectl_config" {
#   description = "kubectl config as generated by the module."
#   value       = module.eks.kubeconfig
# }

# output "config_map_aws_auth" {
#   description = "A kubernetes configuration to authenticate to this EKS cluster."
#   value       = module.eks.config_map_aws_auth
# }

# output "region" {
#   description = "AWS region"
#   value       = "region"
# }

# output "cluster_name" {
#   description = "Kubernetes Cluster Name"
#   value       = local.cluster_name
# }
# resource "aws_security_group" "worker_group_mgmt_one" {
#   name_prefix = "worker_group_mgmt_one"
#   vpc_id      = module.vpc.vpc_id

#   ingress {
#     from_port = 22
#     to_port   = 22
#     protocol  = "tcp"

#     cidr_blocks = [
#       "10.0.0.0/8",
#     ]
#   }
# }

# resource "aws_security_group" "worker_group_mgmt_two" {
#   name_prefix = "worker_group_mgmt_two"
#   vpc_id      = module.vpc.vpc_id

#   ingress {
#     from_port = 22
#     to_port   = 22
#     protocol  = "tcp"

#     cidr_blocks = [
#       "192.168.0.0/16",
#     ]
#   }
# }

# resource "aws_security_group" "all_worker_mgmt" {
#   name_prefix = "all_worker_management"
#   vpc_id      = module.vpc.vpc_id

#   ingress {
#     from_port = 22
#     to_port   = 22
#     protocol  = "tcp"

#     cidr_blocks = [
#       "10.0.0.0/8",
#       "172.16.0.0/12",
#       "192.168.0.0/16",
#     ]
#   }
# }